# CPU

## CPU工作简介

#### CPU职责

CPU主要是一个**存储器+处理器(把数据倒来倒去)**

- 完成基本的逻辑和算术指令

- 访问内存(三级缓存),外部设备(IO)

#### CPU特点

- 主要成本在于三级缓存
- 大部分时间在处理访存和IO(数据的导入导出)
  - 把数据从内存导入显存(访存)
  - 把数据从硬盘导入内存(IO)

![屏幕截图 2023-09-23 105956](C:\Users\nightmare00\Desktop\capture\屏幕截图 2023-09-23 105956.png)

#### 指令概念

- 分类:
  - 算术:`add r3,r4->r4`
  - 访存:`mov r1,[r2]`
  - 控制:`jz end`
- 指令执行时间:**每条指令耗费几个时钟周期*CPU时钟周期长度**

#### 桌面应用

- 轻量级进程,少量线程
- 极少的数值运算指令

|                                 | vim      | ls       |
| ------------------------------- | -------- | -------- |
| 分支Conditional branches        | 13.6%    | 12.5%    |
| 访存Memory accesses             | 45.7%    | 45.7%    |
| **矢量运算Vector instructions** | **1.1%** | **0.2%** |

#### CPU的IPC优化手段

IPC:单个时钟周期执行的指令数量

- 流水线级别增加:CPU同时执行多个指令不同环节,增加环节数量
- 超标量(superscalar)(流水线宽度):CPU同时执行多个指令的同一环节,增加零件个数
- 指令重排(Out-of-Order):减少时间浪费增加并行度

## 2.1 流水线(Pipelining)

#### 流水线执行过程

取指->译码->执行->访存->回写

(同一时刻)指令1:取指,指令2:译码,指令3:执行,指令4:访存,指令5:回写

#### 流水线特点

- 使用指令级并行(instruction-level-parallelism)(ILP)
- 极大地减少时钟周期(significantly-reduced-clock-period)
- 增加一些延迟
- 增加芯片面积:CPU需要引入额外的硬件支持和逻辑电路来支持流水线
  - 流水线寄存器:存储中间结果和指令状态
  - 控制逻辑:处理分支指令的预测,异常处理和流水线冲突等情况

#### 带来的问题

- 怎么处理带有依赖关系的指令
  - 旁路(Bypass):提前取出结果提供给下一条指令使用,不等结果写回等后面操作结束
- 分支:分支的时候不确定下一条指令,需要进行分支预测(大于90%)

- 停滞(stalls):需要上一条指令结果但是上一条指令还没执行完,就停滞

## 2.2 超标量(Superscalar)

#### IPC(Instructions Per Cycle)

- IPC指的是每个时钟周期内完成的指令数量,受两个因素影响
  - 流水线深度(流水线级别数量):同时执行不同指令的不同级别,多划分级别
    - Core2:14级,Pentium4:20级
  - 流水线广度(Superscalar):同时执行不同指令的同一个级别

####　超标量概念

- 同一个级别安排很多个组件:
  - 例:在算术级别安排4个ALU
- 超标量设计实现：超标量处理器的设计目标是在一个时钟周期内同时执行多条指令，以提高指令级并行性。其设计实现方法包括以下关键方面：
  - 指令发射：超标量处理器具有多个指令发射单元，用于从指令流中提取多条指令并发射到执行单元。发射单元负责指令的解码、操作数的获取和指令调度等操作。
  - 执行单元：超标量处理器包含多个执行单元，每个执行单元用于执行不同类型的指令操作。例如，一个执行单元可能专门用于整数运算，另一个执行单元可能用于浮点运算。这样可以同时执行多个指令，提高处理器的执行效率。
  - 数据依赖性检测：超标量处理器需要进行数据依赖性检测，以确保指令之间的依赖关系得到正确处理。此过程通常涉及识别和解决指令之间的数据相关性，以保证指令的执行顺序和结果的正确性。
  - 控制逻辑：超标量处理器需要复杂的控制逻辑来协调指令的发射和执行。这包括指令调度、流水线管理以及处理分支指令等。

## 2.3 乱序执行(Out-of-Order)(OoO)

#### 重排指令

- **让IPC更接近理想状态**:最大可能减少流水线环节的闲置
  - 重排指令可以通过重新安排指令的顺序,使得独立的指令能够并行执行增加CPU指令级并行的能力
  - 隐藏延迟:可以将缓存访问等延迟比较大的指令安排在其他指令执行的空隙中节省时间
- CPU面积和功耗增加

#### 硬件组成:

- 重排缓冲区(Reorder Buffer):记录所有执行中的指令状态
- 发射队列/调度器:选择下一条要执行的指令

## 2.4 存储器架构/层次(Memory Hierarchy)

#### 缓存(Cache)

- 把数据放在尽可能接近的位置,事先先存好数据

#### 存储器分级

- SRAM:使用触发器电路来存储每个比特,速度快,集成度较低(价格贵容量小)
- DRAM:使用电容器来存储每个比特,速度较快,集成度较高,需要刷新和重写

|                      |  延迟   |   带宽    |    大小    |
| -------------------- | :-----: | :-------: | :--------: |
| SRAM (L1, L2, L3)    |  1-2ns  |  200GBps  |   1-20MB   |
| DRAM (memory)        |  70ns   |  20GBps   |   1-32GB   |
| Flash (disk)         | 70-90µs |  200MBps  | 100-1000GB |
| HardDiskDrive (disk) |  10ms   | 1-150MBps | 500-3000GB |

注:内存延迟是CPU访问一次内存要等待的时间,最好在一次内存延迟的时间内获取等同于内存延迟时间*带宽的数据不然访存效率会降低

#### 存储器设计

- 分区:存储器划分为几个区域
  - 例如:代码段,数据段,栈段
  - 分区可以帮助OS更有效地管理系统资源,并且实现内存保护机制
- 控制器:控制多个访问请求优先级调度,优化性能增加带宽

## 2.5 CPU并行性

- 指令级并行(Instruction-level-parallelism):单个时钟周期执行多个指令
  - 流水线
  - 超标量
  - 乱序执行
- 分支预测
  
- 数据级并行(Data-level-parallelism):单个指令处理多个数据(矢量计算)
- 线程级并行(Thread-Level-Parallelism):多核CPU

## 2.6 数据级并行

- 单指令多数据:A+B都是相加,一个向量和一个矩阵相加

```C++
for(int i=0;i<N;i+=4){
    A[i] = B[i]+C[i];
    A[i+1] = B[i+1]+C[i+1];
    A[i+2] = B[i+2]+C[i+2];
    A[i+3] = B[i+3]+C[i+3];
}
```

## 2.7 摩尔定律的墙

#### 功耗墙(Power Wall)

- 随着晶体管密度进一步缩小接近纳米级别,耗能突然极具增加
  - 降低能量利用率
  - 芯片温度过高需要散热

- 为什么不都是单核:时钟频率无法随功耗增加保持线性增长

#### 存储器墙

- 存储器性能跟不上CPU的摩尔定律性能提升速度,存储器速度一直是那么慢
- 存储器性能(访存速度)和CPU处理速度差距越来越大

#### 目前发展方向

- 增加CPU核的数量
- 单核性能不会有太大提升
- 并行处理是方向

# 并行程序设计概述

## 搬砖模型



## 提升效率

- 内存延迟:CPU访问一次内存的时间

# GPU体系架构概述

## 4.1 为什么需要GPU

- FLOPS:`FLoating-point OPerations per second`
  - GPU的FLOPS增长速度远大于CPU
- 应用的需求越来越高
- GPU和CPU的区别
  - CPU当中Out-Of-Order和branch-prediction以及流水线的部分很多,但是ALU相对比较少
  - GPU去除了CPU当中负责流水线优化效率的部分,增加了core的数量

## 4.2 三种方法提升GPU速度

#### 精简内核增加数量

- 单核:在限定一个指令流的情况下提升效率(流水线分支预测等)成本比较高
- 去除CPU当中负责流水线优化效率的部分(乱序,分支预测)
- 增加内核的数量,进一步增加并行性,多个程序片元共享同一个指令流

#### 增加ALU,SIMD

- 一个core内部有多个ALU
- SIMD:单指令多数据,可以执行向量操作
  - SIMD处理并不总是需要显式的SIMD指令:显式指令/标量指令程序自动矢量化 	
- 分支怎么解决?:一些ALU等待(不满足条件),另一些ALU执行(假设16个ALU共享指令流最差情况表现为1/16)

#### 延迟掩藏

- 延迟掩藏概念
  - 把还没有准备好的任务延迟执行
  - 与挂起的区别是挂起有任务切换,而延迟掩藏是事件驱动执行
- 实现方式:回调函数或者监听器等
- 上下文:可以把上下文划分为x份,切换工作的时候直接切换
  - 大的上下文(划分为4个上下文):单个任务上下文占用空间大,但是上下文数量少导致延迟掩藏能力差
  - 小的上下文(划分为18个上下文):数量多,可以实现很多个任务之间的延迟掩藏

## 4.3 实际GPU设计举例

#### My Chip

16个core(取码译码),每个core有8个mul-add-ALU,每个core有4个context

- 同时处理16个指令流
- 并发处理64个指令流(来回切换)
- 并发处理512个程序片元(64个指令流每个指令流中有8个程序片元)

#### GTX 480

- CUDA-core(也叫SP):相当于前面的ALU
- Streaming-MultiProcessor:流多处理器
  - 一个SM里面有多个Streaming-Processor(SP)
  - 一个SM当中可以有多个取指译码单元:可以同时处理多个指令流
  - 一个SM的多个流处理器之间共享一片上下文空间和内存

## 4.4 GPU的存储器设计

- 带宽受限:减少带宽需求,一次性取出数据,计算密集而不是访存密集

# CUDA编程教程

## 5.1 CPU和GPU互动模式

DRAM<->CPU<->i/O<-----------PCIE------------>i/O<->GPU<->GDRAM

CPU和GPU之间通过PCIE总线进行通信,速度非常慢,所以需要有一个显存(GPU的内存)

## 5.2 GPU线程组织模型

- **GRID**(网格):最大的并行执行单位,在**GPU**上面执行

- Block(线程块):线程块是中等粒度的并行执行单位,在Streaming-MultiProcessor上执行
- Warp:一个SIMD的执行单位,包含多个thread且这些thread执行相同的指令
- thread:最小的执行单位,在SP(cuda-core)上被执行

